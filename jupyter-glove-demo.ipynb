{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the sample project for running our GloVe implementation\n",
    "\n",
    "Imports required for running the code. Load the sample data for testing the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from src import context_dictionary\n",
    "from src.prepare_court_data import import_dataframe\n",
    "from pyspark.sql.functions import udf, explode, monotonically_increasing_id\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType, MapType\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "df_opinions_unparsed = spark.read.load('data/wash_state_1000_opinions.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then build the lists of words for each sentence in each document. The end result is a list for each document in which each item is a list of words for each sentence. We chose the nested list so we could preserve sentence boundaries when counting the number of times each word appears in the context of another word. The context does not extend beyond the sentence boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_lists = udf(lambda doc: [\n",
    "    word_tokenize(                                              # NLTK word tokenizer is smarter (can separate contractions)\n",
    "        sentence.translate(                                     # translate can change one character into another\n",
    "            str.maketrans(string.punctuation, ' '*len(string.punctuation))  # a translator that changes punctuation within words\n",
    "            )\n",
    "        ) \n",
    "    for sentence in sent_tokenize(doc.replace('\\n', ' ').strip().lower())],         # bring the documents in divided into sentences\n",
    "    ArrayType(ArrayType(StringType())))                                     # declare nested array of strings for Spark\n",
    "df_words = df_opinions_unparsed.withColumn('sents', token_lists('parsed_text'))\n",
    "#df_words.persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create a vocabulary list to store each unique term from all the documents. This can be used later to map terms to index numbers, but may not be necessary for the current process where the term colocations will be transformed into a graph to improve lookup speeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[words: string, id: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = df_words \\\n",
    "        .withColumn('lists', explode('sents')) \\\n",
    "        .withColumn('words', explode('lists')) \\\n",
    "        .select('words') \\\n",
    "        .distinct() \\\n",
    "        .withColumn('id', monotonically_increasing_id())\n",
    "vocab_list.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build the context dictionary for each term. The terms are counted within a predefined window and within sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[token: string, context: string, sum(count): bigint]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udf_contexts = udf(lambda doc: context_dictionary.context(doc), MapType(StringType(), MapType(StringType(), IntegerType())))\n",
    "df_word_dicts = df_words.withColumn('cooccurrence_dicts', udf_contexts('sents'))\n",
    "df_word_dicts.persist()\n",
    "\n",
    "context_counts = df_word_dicts \\\n",
    "        .select(explode('cooccurrence_dicts').alias('token', 'context')) \\\n",
    "        .select('token', explode('context').alias('context', 'count')) \\\n",
    "        .groupBy(['token', 'context']) \\\n",
    "        .sum('count')\n",
    "context_counts.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
