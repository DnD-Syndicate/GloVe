{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the sample project for running our GloVe implementation\n",
    "\n",
    "Imports required for running the code. Load the sample data for testing the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from itertools import chain\n",
    "from src import context_dictionary\n",
    "from src.prepare_court_data import import_dataframe\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType, MapType\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "df_opinions_unparsed = spark.read.load('data/wash_state_1000_opinions.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then build the lists of words for each sentence in each document. The end result is a list for each document in which each item is a list of words for each sentence. We chose the nested list so we could preserve sentence boundaries when counting the number of times each word appears in the context of another word. The context does not extend beyond the sentence boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_lists = udf(lambda doc: [\n",
    "    word_tokenize(                                              # NLTK word tokenizer is smarter (can separate contractions)\n",
    "        sentence.translate(                                     # translate can change one character into another\n",
    "            str.maketrans(string.punctuation, ' '*len(string.punctuation))  # a translator that changes punctuation within words\n",
    "            )\n",
    "        ) \n",
    "    for sentence in sent_tokenize(doc.replace('\\n', ' ').strip())],         # bring the documents in divided into sentences\n",
    "    ArrayType(ArrayType(StringType())))                                     # declare nested array of strings for Spark\n",
    "df_words = df_opinions_unparsed.withColumn('sents', token_lists('parsed_text'))\n",
    "df_words.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
